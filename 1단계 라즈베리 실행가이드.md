# 🤖 라즈베리파이 AI 음성 비서 "주비스" 구축 가이드

> 라즈베리파이 4B에서 Claude + Gemini를 활용한 AI 음성 비서 만들기

---

## 📋 목차

1. [준비물](#1-준비물)
2. [라즈베리파이 OS 설치](#2-라즈베리파이-os-설치)
3. [SSH 원격 접속 설정](#3-ssh-원격-접속-설정)
4. [Node.js 설치](#4-nodejs-설치)
5. [오디오 장치 설정](#5-오디오-장치-설정)
6. [Python 패키지 설치](#6-python-패키지-설치)
7. [API 키 발급](#7-api-키-발급)
8. [음성 비서 코드 작성](#8-음성-비서-코드-작성)
9. [트러블슈팅](#9-트러블슈팅)
10. [VS Code SSH 연결 (선택)](#10-vs-code-ssh-연결-선택)

---

## 1. 준비물

### 하드웨어
| 장비 | 설명 |
|------|------|
| 라즈베리파이 4B | 8GB 권장 |
| Micro SD 카드 | 32GB 이상 |
| USB 마이크 | 예: MATA STUDIO C10 |
| 스피커 | 3.5mm 잭 또는 USB |
| 전원 어댑터 | 5V 3A USB-C |

### 소프트웨어 & API
| 서비스 | 용도 | 무료 티어 |
|--------|------|-----------|
| 리턴제로 VITO | STT (음성→텍스트) | 10시간 |
| Claude API | AI 대화 (분석/코딩) | 유료 |
| Gemini API | AI 대화 (실시간 검색) | 무료 (결제 등록 필요) |
| Google Cloud TTS | TTS (텍스트→음성) | 월 100만 글자 |

---

## 2. 라즈베리파이 OS 설치

### 2.1 Raspberry Pi Imager 다운로드
https://www.raspberrypi.com/software/

### 2.2 OS 설치
1. Raspberry Pi Imager 실행
2. **OS 선택**: Raspberry Pi OS (64-bit) - Bookworm
3. **저장소 선택**: SD 카드 선택
4. **설정 (톱니바퀴)** 클릭:
   - 호스트이름: `Jubis` (원하는 이름)
   - SSH 활성화: 체크
   - 사용자 이름/비밀번호 설정
   - Wi-Fi 설정 (SSID, 비밀번호)
   - 로케일: Asia/Seoul, kr
5. **쓰기** 클릭

---

## 3. SSH 원격 접속 설정

### 3.1 라즈베리파이 IP 확인
라즈베리파이에서:
```bash
hostname -I
```

또는 공유기 관리 페이지에서 확인

### 3.2 SSH 접속 (Windows)
```bash
ssh 사용자이름@IP주소
# 예: ssh dhwnstjr0@192.168.0.31
```

### 3.3 SSH 접속 (Mac/Linux)
```bash
ssh 사용자이름@IP주소
```

---

## 4. Node.js 설치

```bash
# Node.js 22.x 저장소 추가
curl -fsSL https://deb.nodesource.com/setup_22.x | sudo -E bash -

# Node.js 설치
sudo apt-get install -y nodejs

# 설치 확인
node -v  # v22.x.x
npm -v
```

---

## 5. 오디오 장치 설정

### 5.1 장치 확인

**마이크 확인:**
```bash
arecord -l
```

출력 예시:
```
card 3: C10 [MATA STUDIO C10], device 0: USB Audio [USB Audio]
```

**스피커 확인:**
```bash
aplay -l
```

출력 예시:
```
card 2: Headphones [bcm2835 Headphones], device 0: bcm2835 Headphones
```

### 5.2 마이크 테스트
```bash
# 3초 녹음 후 재생
arecord -D plughw:3,0 -d 3 -f S16_LE -r 44100 test.wav
aplay -D plughw:2,0 test.wav
```

> ⚠️ **주의**: `plughw:3,0`에서 3은 마이크 card 번호, `plughw:2,0`에서 2는 스피커 card 번호입니다. 본인 환경에 맞게 수정하세요.

### 5.3 스피커 테스트
```bash
espeak -v ko "안녕하세요 테스트입니다" --stdout | aplay -D plughw:2,0
```

---

## 6. Python 패키지 설치

```bash
# 시스템 패키지
sudo apt update
sudo apt install -y python3-pip python3-pyaudio portaudio19-dev espeak mpg123

# Python 패키지
pip install requests pyaudio anthropic google-generativeai google-cloud-texttospeech --break-system-packages
```

---

## 7. API 키 발급

### 7.1 리턴제로 (STT)

1. https://vito.ai 접속
2. 회원가입 후 콘솔 접속
3. **API 키 발급**
4. Client ID와 Client Secret 저장

### 7.2 Claude API

1. https://console.anthropic.com 접속
2. API Keys에서 키 생성
3. API 키 저장 (sk-ant-...)

### 7.3 Gemini API

1. https://aistudio.google.com 접속
2. **Get API Key** 클릭
3. API 키 생성 및 저장

> ⚠️ **중요**: Gemini 무료 티어를 사용하려면 **결제 설정 활성화**가 필요합니다!
> - Google AI Studio → 사용량 및 결제 → 결제 설정
> - 결제 등록해도 무료 한도 내에서는 비용 발생 안 함

### 7.4 Google Cloud TTS

1. https://console.cloud.google.com 접속
2. **API 및 서비스** → **라이브러리**
3. "Cloud Text-to-Speech API" 검색 후 **사용** 클릭
4. **사용자 인증 정보** → **서비스 계정** 생성
5. 역할: 편집자 선택
6. **키** 탭에서 JSON 키 다운로드

---

## 8. 음성 비서 코드 작성

### 8.1 프로젝트 폴더 생성
```bash
mkdir ~/ai-assistant
cd ~/ai-assistant
```

### 8.2 Google TTS 키 파일 저장
다운받은 JSON 키 파일을 `google_tts_key.json`으로 저장

### 8.3 메인 코드 작성

`assistant.py` 파일 생성:

```python
import pyaudio
import wave
import requests
import json
import subprocess
import anthropic
import google.generativeai as genai
from google.cloud import texttospeech
import struct
import math
import time
import os

# ============ API 설정 ============
CLIENT_ID = "리턴제로_CLIENT_ID"
CLIENT_SECRET = "리턴제로_CLIENT_SECRET"
CLAUDE_API_KEY = "Claude_API_키"
GEMINI_API_KEY = "Gemini_API_키"

# Google Cloud TTS 설정
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/home/사용자이름/ai-assistant/google_tts_key.json"

genai.configure(api_key=GEMINI_API_KEY)

# ============ 오디오 설정 ============
CHUNK = 4096
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100
WAVE_FILE = "input.wav"
OUTPUT_FILE = "output.mp3"
SILENCE_THRESHOLD = 100  # 환경에 따라 조정 (50~300)
SILENCE_DURATION = 1.5

# 웨이크 워드 (여러 변형 지원)
WAKE_WORDS = ["주비스", "주빌", "유빌", "주비", "쥬비스", "헤이"]

def get_access_token():
    """리턴제로 액세스 토큰 발급"""
    url = "https://openapi.vito.ai/v1/authenticate"
    data = {"client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
    response = requests.post(url, data=data)
    return response.json()["access_token"]

def get_rms(data):
    """오디오 볼륨 레벨 계산"""
    count = len(data) // 2
    shorts = struct.unpack("%dh" % count, data)
    sum_squares = sum(s ** 2 for s in shorts)
    return math.sqrt(sum_squares / count) if count > 0 else 0

def text_to_speech(text):
    """Google Cloud TTS로 음성 출력"""
    print(f"🔊 응답: {text}")
    try:
        client = texttospeech.TextToSpeechClient()
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="ko-KR",
            name="ko-KR-Wavenet-A",
            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
        )
        audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3
        )
        response = client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )
        with open(OUTPUT_FILE, "wb") as out:
            out.write(response.audio_content)
        subprocess.run(["mpg123", "-q", "-a", "plughw:2,0", OUTPUT_FILE])
    except Exception as e:
        print(f"TTS 오류: {e}, espeak로 대체")
        subprocess.run(f'espeak -v ko "{text}" --stdout | aplay -D plughw:2,0', shell=True)

def listen_for_wake_word(token):
    """웨이크 워드 감지"""
    print("\n👂 '헤이 주비스'를 기다리는 중...")
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                    input=True, input_device_index=1, frames_per_buffer=CHUNK)
    frames = []
    recording = False
    silent_chunks = 0
    
    while True:
        data = stream.read(CHUNK, exception_on_overflow=False)
        rms = get_rms(data)
        
        if rms > SILENCE_THRESHOLD:
            if not recording:
                recording = True
                frames = []
            frames.append(data)
            silent_chunks = 0
        elif recording:
            frames.append(data)
            silent_chunks += 1
            if silent_chunks > int(SILENCE_DURATION * RATE / CHUNK):
                stream.stop_stream()
                stream.close()
                p.terminate()
                
                wf = wave.open(WAVE_FILE, 'wb')
                wf.setnchannels(CHANNELS)
                wf.setsampwidth(2)
                wf.setframerate(RATE)
                wf.writeframes(b''.join(frames))
                wf.close()
                
                print("녹음 완료, STT 호출 중...")
                text = speech_to_text(token)
                print(f"STT 결과: {text}")
                
                if text and any(word in text for word in WAKE_WORDS):
                    return True
                else:
                    p = pyaudio.PyAudio()
                    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                                    input=True, input_device_index=1, frames_per_buffer=CHUNK)
                    recording = False
                    frames = []

def record_command():
    """명령어 녹음 (무음 감지로 자동 종료)"""
    print("🎤 말씀하세요...")
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                    input=True, input_device_index=1, frames_per_buffer=CHUNK)
    frames = []
    silent_chunks = 0
    started = False
    
    while True:
        data = stream.read(CHUNK, exception_on_overflow=False)
        rms = get_rms(data)
        
        if rms > SILENCE_THRESHOLD:
            started = True
            frames.append(data)
            silent_chunks = 0
        elif started:
            frames.append(data)
            silent_chunks += 1
            if silent_chunks > int(SILENCE_DURATION * RATE / CHUNK):
                break
    
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    wf = wave.open(WAVE_FILE, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(2)
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()
    print("✅ 녹음 완료!")

def speech_to_text(token):
    """리턴제로 STT API 호출"""
    url = "https://openapi.vito.ai/v1/transcribe"
    headers = {"Authorization": f"Bearer {token}"}
    with open(WAVE_FILE, 'rb') as f:
        files = {"file": f}
        data = {"config": json.dumps({"use_diarization": False})}
        response = requests.post(url, headers=headers, files=files, data=data)
    
    task_id = response.json()["id"]
    result_url = f"https://openapi.vito.ai/v1/transcribe/{task_id}"
    while True:
        result = requests.get(result_url, headers=headers).json()
        if result["status"] == "completed":
            if result["results"]["utterances"]:
                return result["results"]["utterances"][0]["msg"]
            return None
        elif result["status"] == "failed":
            return None
        time.sleep(0.1)

def classify_question(question):
    """Claude로 질문 분류 (순차 처리)"""
    print("🧠 질문 분류 중...")
    client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=10,
        messages=[{"role": "user", "content": f"질문을 분류해. 실시간 정보(날씨,뉴스,주가,시간,검색)면 gemini, 아니면 claude. 한 단어만 답해.\n\n질문: {question}\n답:"}]
    )
    result = message.content[0].text.strip().lower()
    return "gemini" if "gemini" in result else "claude"

def ask_gemini(question):
    """Gemini API 호출 (실시간 정보)"""
    print("🌐 Gemini 검색 중...")
    model = genai.GenerativeModel('gemini-2.0-flash')
    response = model.generate_content(f"친절한 AI 비서로서 짧게 한국어로 답해. 2-3문장.\n\n질문: {question}")
    return response.text

def ask_claude(question):
    """Claude API 호출 (분석/창작)"""
    print("🤖 Claude 생각 중...")
    client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=200,
        messages=[{"role": "user", "content": question}],
        system="친절한 AI 음성 비서. 짧고 자연스럽게 한국어로 2-3문장 답변."
    )
    return message.content[0].text

def main():
    print("=" * 50)
    print("🤖 AI 음성 비서 - 주비스")
    print("=" * 50)
    print("📌 '헤이 주비스'라고 불러주세요!")
    print("=" * 50)
    
    token = get_access_token()
    print("✅ 리턴제로 인증 완료!")
    
    while True:
        try:
            if listen_for_wake_word(token):
                text_to_speech("네, 말씀하세요")
                record_command()
                print("🔄 음성 인식 중...")
                question = speech_to_text(token)
                
                if question:
                    print(f"\n📝 인식된 텍스트: {question}")
                    ai_choice = classify_question(question)
                    print(f"🎯 선택된 AI: {ai_choice}")
                    
                    if ai_choice == "gemini":
                        response = ask_gemini(question)
                    else:
                        response = ask_claude(question)
                    
                    text_to_speech(response)
                else:
                    text_to_speech("죄송해요, 잘 못 들었어요")
        except KeyboardInterrupt:
            print("\n👋 종료합니다!")
            break
        except Exception as e:
            print(f"오류: {e}")
            continue

if __name__ == "__main__":
    main()
```

### 8.4 실행
```bash
python3 assistant.py

# ALSA 경고 숨기고 실행
python3 assistant.py 2>/dev/null
```

---

## 9. 트러블슈팅

### 9.1 Segmentation fault 오류

**원인**: PyAudio 마이크 인덱스가 잘못됨

**해결**:
```bash
# 올바른 마이크 인덱스 확인
python3 -c "
import pyaudio
p = pyaudio.PyAudio()
for i in range(p.get_device_count()):
    info = p.get_device_info_by_index(i)
    if info['maxInputChannels'] > 0:
        print(f'{i}: {info[\"name\"]}')
" 2>/dev/null
```

코드에서 `input_device_index=번호`를 올바른 번호로 수정

### 9.2 Invalid sample rate 오류

**원인**: 마이크가 지원하지 않는 샘플레이트

**해결**: `RATE = 16000`을 `RATE = 44100`으로 변경

### 9.3 Input overflowed 오류

**원인**: 버퍼 크기 부족

**해결**: `CHUNK = 1024`를 `CHUNK = 4096`으로 변경

### 9.4 Gemini 429 오류 (할당량 초과)

**원인**: 무료 티어에서 결제 미등록

**해결**:
1. Google AI Studio 접속
2. 사용량 및 결제 → 결제 탭
3. 결제 설정 활성화 (카드 등록)
4. 무료 한도 내에서는 과금 없음

### 9.5 espeak 소리 안 남

**원인**: 기본 오디오 출력 장치 설정 문제

**해결**:
```bash
# 직접 스피커 지정
espeak -v ko "테스트" --stdout | aplay -D plughw:2,0
```

코드에서도 `plughw:2,0` 형태로 스피커 직접 지정

### 9.6 웨이크 워드 인식 안 됨

**원인 1**: SILENCE_THRESHOLD가 너무 높음

**해결**: 
```bash
# 마이크 볼륨 테스트
python3 -c "
import pyaudio
import struct
import math

p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16, channels=1, rate=44100,
                input=True, input_device_index=1, frames_per_buffer=4096)

print('말해보세요!')
for i in range(30):
    data = stream.read(4096, exception_on_overflow=False)
    count = len(data) // 2
    shorts = struct.unpack('%dh' % count, data)
    rms = math.sqrt(sum(s**2 for s in shorts) / count)
    print(f'볼륨: {int(rms)}')
stream.close()
" 2>/dev/null
```

조용할 때 볼륨 값보다 약간 높게 SILENCE_THRESHOLD 설정

**원인 2**: STT가 발음을 다르게 인식

**해결**: WAKE_WORDS 리스트에 다양한 변형 추가
```python
WAKE_WORDS = ["주비스", "주빌", "유빌", "주비", "쥬비스", "헤이"]
```

### 9.7 ALSA 경고 메시지

```
ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM surround21
Cannot connect to server socket err = No such file or directory
jack server is not running or cannot be started
```

**원인**: 정상적인 경고, 무시 가능

**해결**: 경고 숨기기
```bash
python3 assistant.py 2>/dev/null
```

---

## 10. VS Code SSH 연결 (선택)

터미널 대신 VS Code에서 편하게 개발하기

### 10.1 확장 프로그램 설치
1. VS Code 열기
2. 확장(Extensions) → "Remote - SSH" 검색 → 설치

### 10.2 연결
1. **F1** 또는 **Ctrl+Shift+P**
2. `Remote-SSH: Connect to Host` 입력
3. `+ Add New SSH Host` 선택
4. `ssh 사용자이름@IP주소` 입력 (예: `ssh dhwnstjr0@192.168.0.31`)
5. 설정 파일 선택 (첫 번째)
6. 다시 **F1** → `Remote-SSH: Connect to Host` → IP 선택
7. Linux 선택 → 비밀번호 입력

### 10.3 폴더 열기
연결 후 **폴더 열기** → `/home/사용자이름/ai-assistant` 입력

---

## 📊 시스템 구성도

```
[마이크] → [음성 감지] → ["헤이 주비스" 대기]
                              ↓
                        [웨이크 워드 인식]
                              ↓
                        [TTS: "네, 말씀하세요"]
                              ↓
                        [명령어 녹음]
                              ↓
                        [리턴제로 STT]
                              ↓
                        [Claude 질문 분류]
                              ↓
              ┌───────────────┴───────────────┐
              ↓                               ↓
        [실시간 정보]                   [분석/창작]
              ↓                               ↓
          [Gemini]                       [Claude]
              ↓                               ↓
              └───────────────┬───────────────┘
                              ↓
                     [Google Cloud TTS]
                              ↓
                         [스피커]
```

---

## 🔧 API 비용 정리

| 서비스 | 무료 티어 | 초과 시 |
|--------|-----------|---------|
| 리턴제로 STT | 10시간/월 | 유료 |
| Claude API | 없음 | $3/1M tokens |
| Gemini API | 무료 (결제 등록 필요) | 유료 |
| Google Cloud TTS | 100만 글자/월 | 유료 |

---

## 📝 향후 개선 사항

1. **웨이크 워드 정확도 향상**
   - Porcupine 등 전용 웨이크 워드 엔진 사용
   - 발음 유사도 매칭 알고리즘 적용

2. **TTS 음질 개선**
   - ElevenLabs 등 고품질 TTS 연동

3. **대화 컨텍스트 유지**
   - 이전 대화 내용 기억

4. **ReSpeaker HAT 연동**
   - LED 상태 표시
   - 물리 버튼 활용

---

## 📌 참고 링크

- [리턴제로 VITO](https://vito.ai)
- [Claude API](https://console.anthropic.com)
- [Google AI Studio](https://aistudio.google.com)
- [Google Cloud Console](https://console.cloud.google.com)
- [라즈베리파이 공식](https://www.raspberrypi.com)

---

**작성일**: 2026년 1월 10일  
**환경**: Raspberry Pi 4B 8GB, Raspberry Pi OS 64-bit (Bookworm)
